{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-LE4hUrA0SN"
      },
      "source": [
        "# Predicting House Prices in California with `LinearRegression()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BArvM6eA0SV"
      },
      "source": [
        "In this lab you will start inspect, analyze, visualize house price data from different districts in California, US. After having performed analysis, EDA and some feature engineering, you will build your own `LinearRegression()`  with `SkLearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5h5iSyWA0SX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwLrM1JQA0SY"
      },
      "source": [
        "# Part 1 - Inspection and Cleaning\n",
        "\n",
        "\n",
        "#### Import and Inspect your data\n",
        "\n",
        "Read the `housing.csv` file and make use of some methods to understand your data better. Below is an explanation of the features you are going to work with:\n",
        "\n",
        "1. **longitude:**  geographical coordinate, east to west position of district\n",
        "2. **latitude:**  geographical coordinate, north to south position of district\n",
        "3. **housing_median_age:** the median age of houses in district\n",
        "4. **total_rooms** Sum of all rooms in district\n",
        "5. **total_bedrooms** Sum of all bedrooms in district\n",
        "6. **population:** total population in district\n",
        "7. **households:** total households in district\n",
        "8. **median_income:** median household income in district\n",
        "9. **median_house_value:** median house value in district\n",
        "10. **ocean_proximity:** District´s proximity to the ocean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2WpneUWA0Sb"
      },
      "outputs": [],
      "source": [
        "housing_data = pd.read_csv('housing.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z3gwRM9A0Sb"
      },
      "outputs": [],
      "source": [
        "housing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weslGLo_A0Se"
      },
      "source": [
        "#### Histograms\n",
        "Make histograms of all your numeric columns in order to get a good understanding of the distribution of your data points. What do you see?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul-cotGcA0Se"
      },
      "outputs": [],
      "source": [
        "#selecting num columns\n",
        "num_cols = housing_data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "#hist plots\n",
        "housing_data[num_cols].hist(bins=30, figsize=(15, 10))\n",
        "plt.suptitle(\"Histograms of Numeric Columns\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp-EHSq9A0Sf"
      },
      "source": [
        "#### Let's create some features a tidy up our data\n",
        "\n",
        "1. Locate your NaN values and make a decision on how to handle them. Drop, fill with mean, or something else, it is entirely up to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1RzdzSwA0Sf"
      },
      "outputs": [],
      "source": [
        "# 1. Locate NaN values\n",
        "print(\"Missing values per column before handling:\")\n",
        "print(housing_data.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Handle missing values\n",
        "# Numeric columns: fill NaNs with median\n",
        "numeric_cols = housing_data.select_dtypes(include='number').columns\n",
        "for col in numeric_cols:\n",
        "    if housing_data[col].isnull().any():\n",
        "        housing_data[col].fillna(housing_data[col].median(), inplace=True)"
      ],
      "metadata": {
        "id": "8ICIpRtcRbGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vob2lFOA0Sg"
      },
      "outputs": [],
      "source": [
        "# Categorical columns: fill NaNs with mode\n",
        "categorical_cols = housing_data.select_dtypes(exclude='number').columns\n",
        "for col in categorical_cols:\n",
        "    if housing_data[col].isnull().any():\n",
        "        housing_data[col].fillna(housing_data[col].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. Confirm no more NaNs\n",
        "print(\"\\nMissing values per column after handling:\")\n",
        "print(housing_data.isnull().sum())"
      ],
      "metadata": {
        "id": "LwKygnQwRhwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2FNLAUGA0Sg"
      },
      "source": [
        "2. Create three new columns by using simple arithmetic operations. Create one column with \"rooms per household\", one with \"population per household\",  and one with \"bedrooms per room\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQabQphFA0Si"
      },
      "source": [
        "3. If you check the largest and smallest values of your \"rooms per houshold column\" you will see two outliers and two values that are just wrong. Drop the four values by index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBGL380tA0Si"
      },
      "outputs": [],
      "source": [
        "housing_data['rooms_per_household'] = housing_data['total_rooms'] / housing_data['households']\n",
        "\n",
        "print(\"Two largest rooms_per_household values:\")\n",
        "print(housing_data['rooms_per_household'].nlargest(2))\n",
        "\n",
        "print(\"Two smallest rooms_per_household values:\")\n",
        "print(housing_data['rooms_per_household'].nsmallest(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLdt-onJA0Si"
      },
      "outputs": [],
      "source": [
        "idx = list(housing_data['rooms_per_household'].nlargest(2).index) + \\\n",
        "      list(housing_data['rooms_per_household'].nsmallest(2).index)\n",
        "\n",
        "housing_data_clean = housing_data.drop(index=idx)\n",
        "print(f\"Dropped indices {idx}. New shape: {housing_data_clean.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUKEEuDKA0Sj"
      },
      "source": [
        "# Part 2 - Exploratory Data Analysis\n",
        "\n",
        "\n",
        "\n",
        "#### Let's find out what factors have an influence on our predicting variable\n",
        "\n",
        "1. Let's check out the distribution of our \"median house value\". Visualize your results with 100 bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yARjCJvMA0Sj"
      },
      "outputs": [],
      "source": [
        "# Plot the distribution of median_house_value with 100 bins\n",
        "plt.figure()\n",
        "plt.hist(housing_data['median_house_value'], bins=100)\n",
        "plt.title(\"Distribution of Median House Value\")\n",
        "plt.xlabel(\"Median House Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKk5r6l4A0Sk"
      },
      "source": [
        "2. Check out what variables correlates the most with \"median house value\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-wJRlU8A0Sk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrEWu8XJA0Sk"
      },
      "source": [
        "3. Let's check out the distribution of the column that has the highest correlation to \"median house value\". Visualize your results with 100 bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhgLpTzjA0Sl"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.hist(housing_data['median_income'], bins=100)\n",
        "plt.title(\"Distribution of Median Income\")\n",
        "plt.xlabel(\"Median Income\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Epq8B7dA0Sl"
      },
      "source": [
        "4. Visualize the \"median house value\" and \"median income\" in a jointplot (kind=\"reg\"). What do you see?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f6l3PgRA0Sl"
      },
      "outputs": [],
      "source": [
        "# Define variables\n",
        "x = housing_data['median_income']\n",
        "y = housing_data['median_house_value']\n",
        "\n",
        "# Fit linear regression\n",
        "m, b = np.polyfit(x, y, 1)\n",
        "\n",
        "# Plot scatter + regression line\n",
        "plt.figure()\n",
        "plt.scatter(x, y, s=10)\n",
        "plt.plot(x, m * x + b)\n",
        "plt.title(\"Median House Value vs. Median Income (with Regression Line)\")\n",
        "plt.xlabel(\"Median Income\")\n",
        "plt.ylabel(\"Median House Value\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55RWGyXcA0Sm"
      },
      "source": [
        "5. Make the same visualization as in the above, but, cahnge the kind parameter to \"kde\". What extra information does this type of visualization convey, that the one in the above does not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g747ylv4A0Sm"
      },
      "outputs": [],
      "source": [
        "# Create a joint KDE plot\n",
        "sns.jointplot(data=housing_data, x='median_income', y='median_house_value', kind='kde')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N87nGt-GA0Sm"
      },
      "source": [
        "#### Let's get schwifty with some EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xgezcXKA0Sn"
      },
      "source": [
        "1. Create a new categorical column from the \"median income\" with the following quartiles `[0, 0.25, 0.5, 0.75, 0.95, 1]` and label them like this `[\"Low\", \"Below_Average\", \"Above_Average\", \"High\", \"Very High\"]` and name the column \"income_cat\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FysgxKpA0Sn"
      },
      "outputs": [],
      "source": [
        "# Create income categories based on specified quantile bins\n",
        "housing_data['income_cat'] = pd.qcut(\n",
        "    housing_data['median_income'],\n",
        "    q=[0, 0.25, 0.5, 0.75, 0.95, 1],\n",
        "    labels=[\"Low\", \"Below_Average\", \"Above_Average\", \"High\", \"Very High\"]\n",
        ")\n",
        "\n",
        "# Display the first few rows to confirm the new column\n",
        "print(housing_data[['median_income', 'income_cat']].head(10))\n",
        "\n",
        "# Show counts per category\n",
        "print(\"\\nValue counts for income_cat:\")\n",
        "print(housing_data['income_cat'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePuwynvPA0Sn"
      },
      "source": [
        "2. Using the Seaborn library, plot the count of your new column and set the `hue` to \"ocean_proximity\". What interesting things can you see?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eppqt0UgA0Sn"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Plot count of income categories with hue by ocean proximity\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(\n",
        "    data=housing_data,\n",
        "    x='income_cat',\n",
        "    hue='ocean_proximity'\n",
        ")\n",
        "plt.title(\"Count of Income Categories by Ocean Proximity\")\n",
        "plt.xlabel(\"Income Category\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend(title=\"Ocean Proximity\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNkNlym1A0So"
      },
      "source": [
        "3. Create two barplots where you set \"y=\"median_house_value\" on both, and the x is first \"income cat\" and then \"ocean_proximity\". How does these two graphs complement what you saw in the graph in your previous question?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ4VZYJcA0So"
      },
      "outputs": [],
      "source": [
        "# First barplot: average median_house_value by income_cat\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(data=housing_data, x='income_cat', y='median_house_value', ci=None)\n",
        "plt.title('Average Median House Value by Income Category')\n",
        "plt.xlabel('Income Category')\n",
        "plt.ylabel('Average Median House Value')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSlcOVxIA0TB"
      },
      "outputs": [],
      "source": [
        "# Second barplot: average median_house_value by ocean_proximity\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(data=housing_data, x='ocean_proximity', y='median_house_value', ci=None)\n",
        "plt.title('Average Median House Value by Ocean Proximity')\n",
        "plt.xlabel('Ocean Proximity')\n",
        "plt.ylabel('Average Median House Value')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These two barplots show average house values along two dimensions:\n",
        "\n",
        "By Income Category:\n",
        "\n",
        "A clear upward trend from Low ($125k) to Very High ($440k).\n",
        "\n",
        "This confirms our earlier finding that income is strongly tied to house values—each income bucket has a noticeably different mean price.\n",
        "\n",
        "By Ocean Proximity:\n",
        "\n",
        "Island tops all others at ~$380k (though this category is very small).\n",
        "\n",
        "Next highest is Near Ocean ($250k) and Near Bay ($260k).\n",
        "\n",
        "<1H Ocean is slightly lower (~$240k).\n",
        "\n",
        "Inland is lowest (~$125k).\n",
        "\n",
        "This complements the countplot by showing not just how many properties fall into each proximity–and income–bucket, but how their prices differ on average."
      ],
      "metadata": {
        "id": "ypeCmZmmf5DB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9dSDJygA0TB"
      },
      "source": [
        "4. Create a pivoted dataframe where you have the values of the \"income cat\" column as indices and the values of the \"ocean_proximity\" column as columns. Also drop the \"ISLAND\" column that you'll get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaZkXKSTA0TB"
      },
      "outputs": [],
      "source": [
        "# Create pivoted dataframe of counts\n",
        "pivot_df = housing_data.pivot_table(\n",
        "    index='income_cat',\n",
        "    columns='ocean_proximity',\n",
        "    aggfunc='size',\n",
        "    fill_value=0\n",
        ")\n",
        "\n",
        "# Drop the 'ISLAND' column\n",
        "pivot_df = pivot_df.drop(columns='ISLAND')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2j8-zZvA0TC"
      },
      "source": [
        "5. Turn your pivoted dataframe into a heatmap. The heatmap should have annotations in integer format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-1UcjiWA0TC"
      },
      "outputs": [],
      "source": [
        "# Recreate income_cat column\n",
        "housing_data['income_cat'] = pd.qcut(\n",
        "    housing_data['median_income'],\n",
        "    q=[0, 0.25, 0.5, 0.75, 0.95, 1],\n",
        "    labels=[\"Low\", \"Below_Average\", \"Above_Average\", \"High\", \"Very High\"]\n",
        ")\n",
        "\n",
        "# Create pivoted DataFrame\n",
        "pivot_df = housing_data.pivot_table(\n",
        "    index='income_cat',\n",
        "    columns='ocean_proximity',\n",
        "    aggfunc='size',\n",
        "    fill_value=0\n",
        ").drop(columns='ISLAND')\n",
        "\n",
        "# Prepare data for heatmap\n",
        "data = pivot_df.values\n",
        "rows = pivot_df.index.tolist()\n",
        "cols = pivot_df.columns.tolist()\n",
        "\n",
        "# Plot heatmap\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(data, aspect='auto')\n",
        "\n",
        "# Set tick labels\n",
        "ax.set_xticks(np.arange(len(cols)))\n",
        "ax.set_xticklabels(cols)\n",
        "ax.set_yticks(np.arange(len(rows)))\n",
        "ax.set_yticklabels(rows)\n",
        "\n",
        "# Annotate each cell with integer count\n",
        "for i in range(len(rows)):\n",
        "    for j in range(len(cols)):\n",
        "        ax.text(j, i, int(data[i, j]), ha='center', va='center')\n",
        "\n",
        "# Labels and title\n",
        "ax.set_xlabel(\"Ocean Proximity\")\n",
        "ax.set_ylabel(\"Income Category\")\n",
        "ax.set_title(\"Counts by Income Category and Ocean Proximity\")\n",
        "fig.colorbar(im, ax=ax, orientation='vertical', label='Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIFEoLGzA0TD"
      },
      "source": [
        "# Part 3 - Preparing your Data\n",
        "\n",
        "\n",
        "\n",
        "#### Splitting, Preparing and Engineering some Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX8TQ47kA0TD"
      },
      "source": [
        "1. Let's drop the \"income_cat\" column as it has served its purpose already. We don't need for our model as we already have \"median income\".\n",
        "Not dropping \"incom cat\" will lead to multicolinearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IAdQJFhA0TD"
      },
      "outputs": [],
      "source": [
        "# Confirm drop\n",
        "print(\"Columns after dropping income_cat:\")\n",
        "print(housing_data.columns.tolist())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COvZsZxqA0TE"
      },
      "source": [
        "2. Select your floating point columns and standardize your data by calculating the Z-score. You can apply the `stats.zscore()` method in a lambda function. Save your results to a variable called `z_scored`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhqVQHwDA0TE"
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBJcphv7A0TG"
      },
      "outputs": [],
      "source": [
        "# 1. Select floating-point columns\n",
        "float_cols = housing_data.select_dtypes(include=['float64']).columns\n",
        "\n",
        "# 2. Standardize using Z-score\n",
        "z_scored = housing_data[float_cols].apply(lambda col: stats.zscore(col))\n",
        "\n",
        "# 3. Preview the standardized data\n",
        "print(\"Floating-point columns standardized (first 5 rows):\")\n",
        "print(z_scored.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH8Gaa72A0TG"
      },
      "source": [
        "3. Turn the only categorical columns into dummies. Be vary of the dummy trap, to avoid multicolinearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ90kcioA0TG"
      },
      "outputs": [],
      "source": [
        "# Identify categorical columns\n",
        "cat_cols = housing_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Create dummies while avoiding the dummy trap (drop_first)\n",
        "housing_cat_dummies = pd.get_dummies(housing_data[cat_cols], drop_first=True)\n",
        "\n",
        "# Combine with the numeric data\n",
        "housing_prepared = pd.concat(\n",
        "    [housing_data.drop(columns=cat_cols), housing_cat_dummies],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Preview the prepared DataFrame\n",
        "print(\"Categorical columns converted to dummies (first 5 rows):\")\n",
        "print(housing_prepared.head())\n",
        "\n",
        "print(\"\\nColumns after encoding:\")\n",
        "print(housing_prepared.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMlu9zUGA0TH"
      },
      "source": [
        "4. Save our predicting variable to `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_YNojtyA0TH"
      },
      "outputs": [],
      "source": [
        "y = housing_prepared['median_house_value']\n",
        "\n",
        "# Quick check\n",
        "print(\"Preview of y (target variable):\")\n",
        "print(y.head())\n",
        "print(f\"\\nShape of y: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWWZULMuA0TH"
      },
      "source": [
        "5. Concatenate `z_scored` and `dummies` and drop the predicting variable. Save to the varible `X`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUIN4NWIA0TI"
      },
      "outputs": [],
      "source": [
        "# 1. Concatenate standardized numeric features and dummy variables\n",
        "X = pd.concat([z_scored, housing_cat_dummies], axis=1)\n",
        "\n",
        "# 2. Drop the predicting variable if it ended up in X\n",
        "if 'median_house_value' in X.columns:\n",
        "    X = X.drop('median_house_value', axis=1)\n",
        "\n",
        "# 3. Preview the resulting feature matrix\n",
        "print(\"Preview of X (first 5 rows):\")\n",
        "print(X.head())\n",
        "print(f\"\\nShape of X: {X.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6GeTsEmA0TI"
      },
      "source": [
        "# Part 4 - Machine Learning\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Train, Test, Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXq4a9WnA0TI"
      },
      "source": [
        "1. Import `train_test_split` and split your data accordingly. Choose an appropriate test size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFXMHh9dA0TJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Assuming X and y are already defined\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Preview the split sizes\n",
        "print(\"Training set shape (X_train):\", X_train.shape)\n",
        "print(\"Test set shape (X_test):\", X_test.shape)\n",
        "print(\"Training target shape (y_train):\", y_train.shape)\n",
        "print(\"Test target shape (y_test):\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC0FnN56A0TJ"
      },
      "source": [
        "#### Building and Training our Model\n",
        "\n",
        "2. Build, fit and train a `LinearRegression` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEq0yfQoA0TJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Instantiate and train the model\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "\n",
        "# Display coefficients and intercept\n",
        "coefficients = lin_reg.coef_\n",
        "intercept = lin_reg.intercept_\n",
        "train_r2 = lin_reg.score(X_train, y_train)\n",
        "\n",
        "print(\"Intercept:\", intercept)\n",
        "print(\"Coefficients:\", coefficients)\n",
        "print(\"Training R² score:\", train_r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIs04_fTA0TK"
      },
      "source": [
        "3. In a scatterplot, visualize the y_train on your x-axis and your predictions on the y-axis. How does your training predictions look?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7m5MsqHXA0TK"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Generate predictions on the training set\n",
        "y_train_pred = lin_reg.predict(X_train)\n",
        "\n",
        "# Scatterplot of actual vs. predicted\n",
        "plt.figure()\n",
        "plt.scatter(y_train, y_train_pred, s=10)\n",
        "# Plot 45-degree line for reference\n",
        "min_val = min(y_train.min(), y_train_pred.min())\n",
        "max_val = max(y_train.max(), y_train_pred.max())\n",
        "plt.plot([min_val, max_val], [min_val, max_val], linewidth=2)\n",
        "plt.title(\"Actual vs Predicted Median House Value (Training Set)\")\n",
        "plt.xlabel(\"Actual Median House Value\")\n",
        "plt.ylabel(\"Predicted Median House Value\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9ClFh_PA0TL"
      },
      "source": [
        "4. From the sklearn metrics module, print the mean_squared_error and R^2-score. What does the metrics tell us?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD21SemlA0TL"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoX2ng65A0TM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate predictions on the test set\n",
        "y_test_pred = lin_reg.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test, y_test_pred)\n",
        "rmse = mse ** 0.5\n",
        "r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "# Display the results\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Squared Error (MSE): 4,838,614,541.14\n",
        "\n",
        "Root MSE (RMSE): 69,560.15\n",
        "\n",
        "R² Score: 0.6308"
      ],
      "metadata": {
        "id": "IQIPA0U6jqBr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlWTikNeA0TM"
      },
      "source": [
        "#### Final Predictions\n",
        "\n",
        "1. Now you are ready to make prediction on the test data. Do that and visualize your results in a new scatterplot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHV3YX2WA0TN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate predictions on the test set\n",
        "y_test_pred = lin_reg.predict(X_test)\n",
        "\n",
        "# Scatterplot of actual vs. predicted for test set\n",
        "plt.figure()\n",
        "plt.scatter(y_test, y_test_pred, s=10)\n",
        "# Plot 45-degree reference line\n",
        "min_val = min(y_test.min(), y_test_pred.min())\n",
        "max_val = max(y_test.max(), y_test_pred.max())\n",
        "plt.plot([min_val, max_val], [min_val, max_val], color='orange', linewidth=2)\n",
        "plt.title(\"Actual vs Predicted Median House Value (Test Set)\")\n",
        "plt.xlabel(\"Actual Median House Value\")\n",
        "plt.ylabel(\"Predicted Median House Value\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brKjxAMhA0TN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V85HuXJUA0TN"
      },
      "source": [
        "2. Print the mean_squared_error and R^2-score again. What has happened?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_1b69_eA0TO"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Recompute metrics on the test set\n",
        "mse_new = mean_squared_error(y_test, y_test_pred)\n",
        "rmse_new = mse_new ** 0.5\n",
        "r2_new = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse_new:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_new:.2f}\")\n",
        "print(f\"R² Score: {r2_new:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE: 4,838,614,541.14\n",
        "\n",
        "RMSE: 69,560.15\n",
        "\n",
        "R²: 0.6308"
      ],
      "metadata": {
        "id": "dz7FG38pj_-n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxr2cTSxA0TO"
      },
      "source": [
        "3. There is another metric called Root mean squared error, Which is the square root of the MSE. Calculate the RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXy1pI9BA0TP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Calculate RMSE directly with squared=False\n",
        "rmse = mean_squared_error(y_test, y_test_pred)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zr4cvfu5kHwn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znmlhc5sA0TP"
      },
      "source": [
        "# Bonus Questions 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWKzwTiDA0TP"
      },
      "source": [
        "1. Create a dataframe with two columns, one consisting of the y_test and one of your model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mxt7sc_MA0TQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CQgtqivA0TQ"
      },
      "source": [
        "2. Make a series of of your new dataframe, by calculating the predicted error in absolut numbers. Save this series to variable name `absolute_errors`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqLDO1rIA0TQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKlOcz8QA0TR"
      },
      "source": [
        "3. If you take the mean of your series, you will get the mean absolute errors, which is another metric for Linear Regressions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6CcFqrgA0TR"
      },
      "source": [
        "# Bonus Question 2 - Build a Random Forest Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g4WP97hA0TR"
      },
      "source": [
        "1. Build, fit and train a `RandomForestRegressor` model. Do this by following the same staps that you followed when building your `LinearRegression`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWGOSfaSA0TS"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6izqVsHsA0TS"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Instantiate the RandomForestRegressor\n",
        "rf_reg = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit the model on the training data\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions\n",
        "y_train_pred_rf = rf_reg.predict(X_train)\n",
        "y_test_pred_rf = rf_reg.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "train_r2_rf = r2_score(y_train, y_train_pred_rf)\n",
        "test_r2_rf = r2_score(y_test, y_test_pred_rf)\n",
        "mse_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
        "rmse_rf = mse_rf ** 0.5\n",
        "\n",
        "# Display results\n",
        "print(f\"Random Forest Training R²: {train_r2_rf:.4f}\")\n",
        "print(f\"Random Forest Test R²: {test_r2_rf:.4f}\")\n",
        "print(f\"Random Forest RMSE: {rmse_rf:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}